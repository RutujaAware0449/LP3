***Assignment_no_1***


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from geopy.distance import geodesic
     

data = pd.read_csv('uber.csv')
     

data.describe()
     
Unnamed: 0	fare_amount	pickup_longitude	pickup_latitude	dropoff_longitude	dropoff_latitude	passenger_count
count	1.784800e+04	17847.000000	17847.000000	17847.000000	17847.000000	17847.000000	17847.000000
mean	2.765310e+07	11.417429	-72.595005	39.951854	-72.580938	39.952298	1.672830
std	1.599173e+07	10.173691	11.458450	6.095753	10.197475	6.096021	1.295875
min	4.800000e+01	2.500000	-748.016667	-74.009697	-75.350437	-74.008745	0.000000
25%	1.383501e+07	6.000000	-73.992000	40.734977	-73.991591	40.733933	1.000000
50%	2.755475e+07	8.500000	-73.981823	40.752377	-73.980073	40.752884	1.000000
75%	4.140304e+07	12.500000	-73.967328	40.767152	-73.963307	40.768318	2.000000
max	5.542169e+07	350.000000	40.770667	41.366138	40.828377	41.366138	6.000000

data = data.dropna()
     

def calculate_distance(row):
    pickup = (row['pickup_latitude'], row['pickup_longitude'])
    dropoff = (row['dropoff_latitude'], row['dropoff_longitude'])
    return geodesic(pickup, dropoff).km
     

data['distance'] = data.apply(calculate_distance, axis=1)
     

data = data[data['distance'] > 0.1]

# Drop unnecessary columns if needed (such as 'Unnamed: 0')
data = data.drop(['Unnamed: 0'], axis=1)

# Boxplot visualization of fare amount to check for outliers
sns.boxplot(x=data['fare_amount'])
plt.show()
     
/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.
  positions = grouped.grouper.result_index.to_numpy(dtype=float)


# Using IQR to detect outliers in 'fare_amount'
Q1 = data['fare_amount'].quantile(0.25)
Q3 = data['fare_amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out outliers
data = data[(data['fare_amount'] >= lower_bound) & (data['fare_amount'] <= upper_bound)]

# Visualize the distribution after outlier removal
sns.boxplot(x=data['fare_amount'])
plt.show()

     
/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.
  positions = grouped.grouper.result_index.to_numpy(dtype=float)


# Check for non-numeric columns
non_numeric_cols = data.select_dtypes(exclude=[np.number]).columns
print(non_numeric_cols)

     
Index(['key', 'pickup_datetime'], dtype='object')

data_numeric = data.select_dtypes(include=[np.number])

     

correlation_matrix = data_numeric.corr()

# Visualize correlation matrix
plt.figure(figsize=(10,6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

     


from sklearn.linear_model import LinearRegression

# Select features and target variable
X = data[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'distance']]
y = data['fare_amount']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Linear Regression model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# Predictions
y_pred_lin = lin_reg.predict(X_test)

# Evaluation metrics
r2_lin = r2_score(y_test, y_pred_lin)
rmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))

print(f"Linear Regression R2: {r2_lin}")
print(f"Linear Regression RMSE: {rmse_lin}")

     
Linear Regression R2: -0.0015022270203508548
Linear Regression RMSE: 4.109921860944951

from sklearn.ensemble import RandomForestRegressor

# Initialize and train the Random Forest model
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)

# Predictions
y_pred_rf = rf_reg.predict(X_test)

# Evaluation metrics
r2_rf = r2_score(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))

print(f"Random Forest R2: {r2_rf}")
print(f"Random Forest RMSE: {rmse_rf}")

     
Random Forest R2: 0.6929280644247466
Random Forest RMSE: 2.2757663344394623


***Assignment_No_2***



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics

     

df=pd.read_csv('emails.csv')
df.head()
df.columns
df.isnull().sum()
df.dropna(inplace = True)
df.drop(['Email No.'],axis=1,inplace=True)
X = df.drop(['Prediction'],axis = 1)
y = df['Prediction']
from sklearn.preprocessing import scale
X = scale(X)
# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,
random_state = 42)
     

print(X_tfidf.shape)
print(y.shape)
     
(3001, 2974)
(5172,)

# Align the labels to the features
y_aligned = y[:3001]  # Subset 'y' to match the number of rows in X_tfidf

     

print(X_tfidf.shape)  # Should be (3001, 2974)
print(y_aligned.shape)  # Should be (3001,)

     
(3001, 2974)
(3001,)

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_aligned, test_size=0.2, random_state=42)
     

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print("Prediction",y_pred)
print("KNN accuracy = ",metrics.accuracy_score(y_test,y_pred))
print("Confusion matrix",metrics.confusion_matrix(y_test,y_pred))

     
Prediction [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0]
KNN accuracy =  0.718801996672213
Confusion matrix [[432   0]
 [169   0]]

model = SVC(C = 1)
# fit
model.fit(X_train, y_train)
# predict
y_pred = model.predict(X_test)
metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)
print("SVM accuracy = ",metrics.accuracy_score(y_test,y_pred))
     
SVM accuracy =  0.718801996672213


 ***Assignment_no_3***



# Step 1: Import Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

data = pd.read_csv("Churn_Modelling.csv")
X = data.iloc[:, 3:13]
y = data.iloc[:, 13]
X = pd.get_dummies(X, drop_first=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = Sequential()
model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

y_pred = model.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred_classes)
print("Accuracy Score: ", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred_classes)
print("Confusion Matrix: \n", conf_matrix)

     
Epoch 1/50
/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
250/250 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.7878 - loss: 0.4898
Epoch 2/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8410 - loss: 0.3790
Epoch 3/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8537 - loss: 0.3499
Epoch 4/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8595 - loss: 0.3418
Epoch 5/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8575 - loss: 0.3451
Epoch 6/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8656 - loss: 0.3241
Epoch 7/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8698 - loss: 0.3191
Epoch 8/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8647 - loss: 0.3278
Epoch 9/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8669 - loss: 0.3252
Epoch 10/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8632 - loss: 0.3263
Epoch 11/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8688 - loss: 0.3181
Epoch 12/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8687 - loss: 0.3193
Epoch 13/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8697 - loss: 0.3111
Epoch 14/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.8739 - loss: 0.3053
Epoch 15/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8696 - loss: 0.3076
Epoch 16/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8744 - loss: 0.3012
Epoch 17/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8762 - loss: 0.2998
Epoch 18/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8775 - loss: 0.3056
Epoch 19/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8697 - loss: 0.3036
Epoch 20/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8771 - loss: 0.3033
Epoch 21/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8721 - loss: 0.2934
Epoch 22/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8696 - loss: 0.2998
Epoch 23/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8823 - loss: 0.2939
Epoch 24/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8722 - loss: 0.2938
Epoch 25/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8770 - loss: 0.2905
Epoch 26/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8799 - loss: 0.2834
Epoch 27/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8779 - loss: 0.2837
Epoch 28/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8759 - loss: 0.2919
Epoch 29/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8819 - loss: 0.2810
Epoch 30/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8777 - loss: 0.2897
Epoch 31/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8800 - loss: 0.2844
Epoch 32/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8793 - loss: 0.2781
Epoch 33/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8819 - loss: 0.2732
Epoch 34/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8822 - loss: 0.2779
Epoch 35/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8844 - loss: 0.2749
Epoch 36/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.8808 - loss: 0.2802
Epoch 37/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.8778 - loss: 0.2831
Epoch 38/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8835 - loss: 0.2729
Epoch 39/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8871 - loss: 0.2717
Epoch 40/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8839 - loss: 0.2754
Epoch 41/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8779 - loss: 0.2761
Epoch 42/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8942 - loss: 0.2575
Epoch 43/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8880 - loss: 0.2659
Epoch 44/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8864 - loss: 0.2720
Epoch 45/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8871 - loss: 0.2678
Epoch 46/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8809 - loss: 0.2683
Epoch 47/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8871 - loss: 0.2676
Epoch 48/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8947 - loss: 0.2520
Epoch 49/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8958 - loss: 0.2531
Epoch 50/50
250/250 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8899 - loss: 0.2566
63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step
Accuracy Score:  0.8485
Confusion Matrix: 
 [[1494  113]
 [ 190  203]]


   ****Assignment_no_4***


import numpy as np
import matplotlib.pyplot as plt

# Step 1: Define the function and its derivative
def function(x):
    return (x + 3)**2

def gradient(x):
    return 2 * (x + 3)

# Step 2: Implement the Gradient Descent Algorithm
def gradient_descent(starting_x, learning_rate, num_iterations):
    x = starting_x
    x_values = []  # To store x values for plotting

    for i in range(num_iterations):
        grad = gradient(x)
        x = x - learning_rate * grad  # Update x using gradient descent rule
        x_values.append(x)

        # Stop if the change in x is very small (convergence)
        if abs(grad) < 1e-6:
            print(f"Converged at iteration {i}")
            break

    return x, x_values

# Step 3: Set initial values
starting_x = 2  # Initial starting point x = 2
learning_rate = 0.1  # Set a learning rate
num_iterations = 100  # Number of iterations

# Step 4: Perform Gradient Descent
final_x, x_values = gradient_descent(starting_x, learning_rate, num_iterations)

# Step 5: Print the final result
print("Local minimum occurs at x =", final_x)

# Step 6: Plot the function and the gradient descent steps
x_range = np.linspace(-10, 5, 100)
y_range = function(x_range)

plt.plot(x_range, y_range, label="y = (x + 3)^2")
plt.scatter(x_values, function(np.array(x_values)), color='red', label="Gradient Descent Steps")
plt.title("Gradient Descent to find Local Minima")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.show()

     
Converged at iteration 73
Local minimum occurs at x = -2.9999996630006667



     ***Assignment_no_6***


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Step 1: Load the dataset
data = pd.read_csv('sales_data_sample.csv', encoding='ISO-8859-1')  # or 'windows-1252'

data_clean = data[['SALES', 'QUANTITYORDERED', 'PRICEEACH']].dropna()

# Step 3: Scale the data to normalize the features (K-Means works better with scaled data)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_clean)

# Step 4: Apply K-Means Clustering and Determine Optimal Clusters using Elbow Method
inertia = []
K = range(1, 11)  # Test for k values from 1 to 10

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)  # Inertia is the sum of squared distances of samples to their closest cluster center

# Step 5: Plot the Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(K, inertia, 'bo-', markersize=8)
plt.xlabel('Number of clusters, k')
plt.ylabel('Inertia (Sum of squared distances)')
plt.title('Elbow Method For Optimal k')
plt.grid(True)
plt.show()

# Step 6: Fit the final model with the optimal number of clusters (based on the elbow curve)
# Let's assume from the elbow method, the optimal k is 3

print("\n")

optimal_k = 3
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans_final.fit_predict(data_scaled)

# Step 7: Add cluster labels to the original dataset
data_clean['Cluster'] = clusters

# Step 8: Visualize the clusters (for 2D features, if possible)
plt.figure(figsize=(10, 6))
plt.scatter(data_clean['SALES'], data_clean['QUANTITYORDERED'], c=clusters, cmap='viridis', s=100)
plt.title(f'K-Means Clustering with {optimal_k} clusters')
plt.xlabel('Sales')
plt.ylabel('Quantity Ordered')
plt.grid(True)
plt.show()

# Step 9: Check the cluster centers
print("Cluster Centers:\n", kmeans_final.cluster_centers_)

     



Cluster Centers:
 [[-0.83846133  0.00340709 -1.31519107]
 [ 1.10107381  0.89846585  0.64278204]
 [-0.2525054  -0.79188456  0.55612886]]

***Assignment no 5***
# Diabetes classification using KNN

import pandas as pd
import numpy as np

data = pd.read_csv("https://raw.githubusercontent.com/sahil-gidwani/ML/main/dataset/diabetes.csv")
data.head()

data.isnull().any()

data.describe().T

# Glucose, BloodPressure, SkinThickness, Insulin, BMI columns have values 0 which does not make sense, hence are missing values
data_copy = data.copy(deep = True)
data_copy[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = data_copy[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN)
data_copy.isnull().sum()

# To fill these Nan values the data distribution needs to be understood
p = data.hist(figsize = (20, 20))

data_copy['Glucose'].fillna(data_copy['Glucose'].mean(), inplace = True)
data_copy['BloodPressure'].fillna(data_copy['BloodPressure'].mean(), inplace = True)
data_copy['SkinThickness'].fillna(data_copy['SkinThickness'].median(), inplace = True)
data_copy['Insulin'].fillna(data_copy['Insulin'].median(), inplace = True)
data_copy['BMI'].fillna(data_copy['BMI'].median(), inplace = True)

p = data_copy.hist(figsize = (20, 20))

p = data.Outcome.value_counts().plot(kind = "bar")

# The above graph shows that the data is biased towards datapoints having outcome value as 0 where it means that diabetes was not present actually. The number of non-diabetics is almost twice the number of diabetic patients
import seaborn as sns
p = sns.pairplot(data_copy, hue = 'Outcome')

import matplotlib.pyplot as plt
plt.figure(figsize = (12, 10))
p = sns.heatmap(data.corr(), annot = True, cmap ='RdYlGn')

plt.figure(figsize = (12, 10))
p = sns.heatmap(data_copy.corr(), annot = True, cmap ='RdYlGn')

# StandardScaler is a data preprocessing technique commonly used in machine learning and statistics to scale or standardize the features (variables) of a dataset. It transforms the data in such a way that the scaled features have a mean of 0 and a standard deviation of 1. This process is also known as z-score normalization or standardization.
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X = pd.DataFrame(sc_X.fit_transform(data_copy.drop(["Outcome"], axis = 1)), columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])

X.head()

Y = data_copy.Outcome

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 1/3, random_state = 42, stratify = Y)

from sklearn.neighbors import KNeighborsClassifier

train_scores = []
test_scores = []
best_k = None
best_test_score = 0.0

# Choose an odd number to avoid tie situations
for k in range(1, 30, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, Y_train)
    # knn.score() is a meausre of accuracy = TP + TN / TP + TN + FP + FN
    train_score = knn.score(X_train, Y_train)
    test_score = knn.score(X_test, Y_test)
    train_scores.append(train_score)
    test_scores.append(test_score)

    # Check if the current k results in a higher accuracy
    if test_score > best_test_score:
        best_k = k
        best_test_score = test_score

print(f"Best k: {best_k}")

plt.figure(figsize = (12, 5))
plt.title('Accuracy vs k')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy Score')
p = sns.lineplot(x = range(1, 30, 2), y = train_scores, marker = '*', label = 'Train Score', markers = True)
p = sns.lineplot(x = range(1, 30, 2), y = test_scores, marker = 'o', label = 'Test Score', markers = True)

# Setup a knn classifier with best_k neighbors
knn = KNeighborsClassifier(n_neighbors=best_k)

knn.fit(X_train, Y_train)
knn.score(X_test, Y_test)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score

Y_pred = knn.predict(X_test)
cnf_matrix = confusion_matrix(Y_test, Y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix, display_labels=knn.classes_)
disp.plot()

def model_evaluation(y_test, y_pred, model_name):
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    f2 = fbeta_score(y_test, y_pred, beta = 2.0)

    results = pd.DataFrame([[model_name, acc, prec, rec, f1, f2]],
                       columns = ["Model", "Accuracy", "Precision", "Recall",
                                 "F1 SCore", "F2 Score"])
    results = results.sort_values(["Precision", "Recall", "F2 Score"], ascending = False)
    return results

model_evaluation(Y_test, Y_pred, "KNN")

from sklearn.metrics import auc, roc_auc_score, roc_curve

# predict_proba() is used to predict the class probabilities
# [:,-1]: This slice notation selects the last column of the probability matrix, which corresponds to the probability of the positive class
Y_pred_proba = knn.predict_proba(X_test)[:,-1]
fpr, tpr, threshold = roc_curve(Y_test, Y_pred_proba)

classifier_roc_auc = roc_auc_score(Y_test, Y_pred_proba)
plt.plot([0,1], [0,1], label = "(area = 0.5)")

plt.plot(fpr, tpr, label ='KNN (area = %0.2f)' % classifier_roc_auc)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title(f'Knn(n_neighbors = {best_k}) ROC curve')
plt.legend(loc = "lower right", fontsize = "medium")
plt.show()

"""
Data normalization, also known as feature scaling, is a preprocessing technique used to transform data into a common scale or range. Normalizing data is essential, especially in machine learning algorithms that are sensitive to the magnitude of features. Here are some common methods of data normalization:

1. **Min-Max Scaling (Normalization):**
   - Also known as min-max normalization.
   - Scales features to a specific range, usually between 0 and 1.
   - The formula for min-max scaling is: 
     - [X_normalized = {X - X_{min}}/{X_{max} - X_{min}}]
   - It preserves the relationships between data points.
   - Suitable for algorithms that rely on feature values within a specific range, like neural networks.

2. **Z-Score Standardization (Standardization):**
   - Also known as standardization or z-score normalization.
   - Scales features to have a mean (\(\mu\)) of 0 and a standard deviation (\(\sigma\)) of 1.
   - The formula for standardization is:
     - [X_standardized = {X - mu}{sigma}]
   - It centers the data around the mean and scales it by the standard deviation.
   - Useful for algorithms that assume normally distributed data, like many statistical methods and PCA (Principal Component Analysis).

3. **Robust Scaling:**
   - Similar to min-max scaling but more resistant to outliers.
   - It uses the interquartile range (IQR) instead of the minimum and maximum values.
   - The formula for robust scaling is:
     - \[X_scaled = \frac{X - Q_1}{Q_3 - Q_1}\]
     - Where \(Q_1\) is the 25th percentile, and \(Q_3\) is the 75th percentile of the data.
   - Useful when dealing with data containing outliers.

4. **Log Transformation:**
   - Applies a logarithmic function to the data.
   - Useful when the data has a skewed distribution and you want to make it more symmetrical.
   - Often used with data containing exponential growth or decay.

5. **Box-Cox Transformation:**
   - A family of power transformations that includes the logarithm as a special case.
   - It optimizes the transformation based on the data to make it as close to a normal distribution as possible.
   - Applicable to data with varying degrees of non-linearity.

6. **Quantile Transformation:**
   - Maps data to a uniform or normal distribution.
   - Each feature's values are replaced with their respective quantiles.
   - Useful for converting data to a form suitable for machine learning models that make distributional assumptions.

7. **Unit Vector Transformation (Vector Normalization):**
   - Scales each feature to have a Euclidean length (L2 norm) of 1.
   - Useful for algorithms sensitive to the magnitude of vectors, like K-nearest neighbors.

The choice of data normalization method depends on the specific characteristics of your data and the requirements of your machine learning algorithm. It's important to consider the distribution of your data, the presence of outliers, and the assumptions of the models you're using when selecting a normalization technique.
"""
"""
The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) are important tools for evaluating the performance of binary classification models. They help assess how well a model can distinguish between two classes (e.g., positive and negative) by analyzing the trade-off between sensitivity and specificity.

Here's a detailed explanation of the ROC AUC curve:

1. **Binary Classification Background**: In binary classification, you typically have two classes, often referred to as "positive" and "negative." The goal is to develop a model that can classify data points into one of these two categories.

2. **ROC Curve (Receiver Operating Characteristic Curve)**: The ROC curve is a graphical representation of the performance of a binary classifier at various classification thresholds. It plots two important metrics:
   
   - **Sensitivity (True Positive Rate, TPR)**: Sensitivity is the fraction of actual positive cases that the model correctly identifies as positive. It is calculated as TPR = TP / (TP + FN), where TP is True Positives and FN is False Negatives. Sensitivity measures the model's ability to correctly detect positive cases.

   - **1 - Specificity (False Positive Rate, FPR)**: Specificity is the fraction of actual negative cases that the model incorrectly classifies as positive. It is calculated as FPR = FP / (FP + TN), where FP is False Positives and TN is True Negatives. 1 - Specificity measures the model's ability to avoid classifying negative cases as positive.

3. **Threshold Variation**: The ROC curve is constructed by varying the classification threshold used by the model. At each threshold, it calculates the TPR and FPR, resulting in a point on the ROC curve. By varying the threshold, you can observe how the model's performance changes.

4. **AUC (Area Under the Curve)**: The AUC is a single scalar value that summarizes the entire ROC curve. It quantifies the overall performance of a binary classification model. The AUC represents the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. An AUC of 0.5 indicates that the model performs no better than random guessing, while an AUC of 1.0 signifies a perfect classifier.

   - An AUC of 0.5: The model has no discriminative ability, equivalent to random guessing.
   - An AUC less than 0.5: The model's performance is worse than random guessing.
   - An AUC between 0.5 and 1.0: The model is better than random guessing, with a higher AUC indicating better performance.

5. **Interpreting ROC AUC**: A model with a higher AUC is generally better at distinguishing between the two classes. It means that as you vary the classification threshold, the model is more likely to assign higher probabilities to true positives than to true negatives.

6. **Choosing the Threshold**: The ROC curve does not provide an optimal threshold directly. Depending on the specific use case, you may select a threshold that balances sensitivity and specificity or meets specific requirements.

In summary, the ROC AUC curve provides a visual and quantitative way to assess the performance of binary classification models by considering their ability to discriminate between positive and negative cases across various classification thresholds. A higher AUC indicates better model performance.
"""
"""
K-Nearest Neighbors (KNN) is a simple and versatile classification and regression machine learning algorithm. It is used for both classification and regression tasks. The key idea behind KNN is to predict the class or value of a data point based on the majority class or average value of its neighboring data points.

Here's how KNN works:

1. **Training Phase:** In the training phase, KNN doesn't learn a specific model. Instead, it memorizes the entire training dataset.

2. **Prediction Phase (Classification):** When you want to classify a new data point, KNN looks at the K-nearest neighbors (data points with the most similar features) from the training dataset. It uses a distance metric (typically Euclidean distance, Manhattan distance, etc.) to measure the similarity between data points.

   - If it's a classification task (predicting a class label), KNN counts the number of neighbors in each class. The class with the majority of neighbors becomes the predicted class for the new data point.

3. **Prediction Phase (Regression):** In regression tasks, KNN predicts a continuous value. Instead of counting class labels, KNN calculates the average (or weighted average) of the target values of the K-nearest neighbors.

   - For example, if you want to predict a house's price, KNN will average the prices of the K-nearest neighboring houses.

4. **Choosing K:** The choice of the value K is crucial in KNN. It's typically an odd number to avoid ties, but the optimal K depends on your dataset and problem. A smaller K (e.g., 1 or 3) makes the model more sensitive to noise, while a larger K provides a smoother decision boundary.

KNN's strengths and weaknesses:

**Strengths:**
- KNN is simple and easy to implement.
- It can be used for both classification and regression.
- It doesn't assume a particular form for the decision boundary.
- It can be effective for multi-class classification problems.

**Weaknesses:**
- KNN can be computationally expensive, especially for large datasets.
- It's sensitive to the choice of the distance metric and the value of K.
- The curse of dimensionality: KNN becomes less effective as the number of dimensions (features) increases.
- It doesn't handle imbalanced datasets well, where one class has significantly more instances than the others.

KNN is often used for its simplicity and can serve as a baseline model for classification and regression tasks. However, to make the most of it, you need to carefully choose the right distance metric, K value, and handle issues like scaling features and handling missing data, if applicable.
"""

     